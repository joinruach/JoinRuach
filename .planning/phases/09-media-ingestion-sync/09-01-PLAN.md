# Phase 9: Media Ingestion & Sync - Execution Plan

**Goal:** Enable "drag & drop 3 videos → auto-sync → confidence + offsets → operator approves → ready for EDL/rendering"

**Non-negotiables:**
1. Sync correctness (broken sync breaks everything downstream)
2. Preview usability (smooth proxy scrubbing for operators)
3. Storage patterns (cost-effective + deterministic + scalable)

---

## Work Breakdown (Build Order)

This plan breaks Phase 9 into 5 sequential tracks that build on each other.

---

### Track A: Data Model (Strapi)

**Deliverable:** Strapi content types to represent recording sessions + assets + derived media + sync results

**Implementation:**

Create or extend these Strapi content types:

1. **recording-session**
   - Fields:
     - `title` (string, required)
     - `date` (date, required)
     - `location` (string)
     - `notes` (text)
     - `projectId` (string, required)
     - `status` (enum: draft | ingesting | syncing | needs-review | ready)
   - Relations:
     - `assets` (hasMany → media-asset)
     - `syncResult` (hasOne → sync-result)

2. **media-asset**
   - Fields:
     - `cameraLabel` (enum: A | B | C)
     - `originalFilename` (string)
     - `originalUrl` (string)
     - `durationMs` (integer)
     - `fps` (integer)
     - `resolution` (string)
     - `codec` (string)
     - `status` (enum: uploading | ingesting | ready | failed)
   - Relations:
     - `session` (belongsTo → recording-session)
     - `derivatives` (hasMany → derived-media)

3. **derived-media**
   - Fields:
     - `kind` (enum: mezzanine | proxy | audio-wav | waveform | thumbnail)
     - `url` (string, required)
     - `sizeBytes` (biginteger)
     - `checksum` (string)
     - `createdAt` (datetime, auto)
   - Relations:
     - `asset` (belongsTo → media-asset)

4. **sync-result**
   - Fields:
     - `masterCamera` (enum: A | B | C)
     - `offsetsMs` (json) - e.g., `{ "A": 0, "B": 1234, "C": -532 }`
     - `confidence` (json) - e.g., `{ "B": 12.4, "C": 8.1 }`
     - `method` (enum: xcorr | fingerprint | hybrid)
     - `operatorStatus` (enum: pending | approved | corrected)
     - `operatorOffsetsMs` (json, optional)
     - `debugArtifacts` (json, optional) - URLs to correlation plots
     - `computedAt` (datetime)
     - `approvedAt` (datetime, optional)
   - Relations:
     - `session` (belongsTo → recording-session)

**Acceptance Criteria:**
- [ ] Can create session in Strapi admin
- [ ] Can attach 3 assets to session
- [ ] Can create derived-media for each asset
- [ ] Can store sync-result with offsets and confidence
- [ ] All relations work correctly (cascade deletes if needed)

**Files:**
- `apps/studio-strapi/src/api/recording-session/content-types/recording-session/schema.json`
- `apps/studio-strapi/src/api/media-asset/content-types/media-asset/schema.json`
- `apps/studio-strapi/src/api/derived-media/content-types/derived-media/schema.json`
- `apps/studio-strapi/src/api/sync-result/content-types/sync-result/schema.json`

---

### Track B: Storage + Naming (R2)

**Deliverable:** Deterministic R2 object keys and lifecycle rules

**Implementation:**

1. **Define R2 key schema:**

```typescript
// apps/studio-api/src/services/storage/object-naming.ts

interface AssetMetadata {
  projectId: string;
  shootDate: Date;
  angleNumber: number; // 1-3 for cameras A-C
  assetType: 'original' | 'proxy' | 'mezzanine' | 'audio';
  version?: number;
}

export function buildR2ObjectKey(metadata: AssetMetadata, extension: string): string {
  const { projectId, shootDate, angleNumber, assetType, version = 1 } = metadata;

  // ISO 8601 date: YYYYMMDD
  const dateStr = shootDate.toISOString().slice(0, 10).replace(/-/g, '');

  // Filename: angle-001_v001.mp4
  const fileName = `angle-${angleNumber.toString().padStart(3, '0')}_v${version.toString().padStart(3, '0')}${extension}`;

  // Path: projects/proj-abc123/20260203/originals/angle-001_v001.mp4
  return [
    'projects',
    projectId,
    dateStr,
    `${assetType}s`, // pluralized: originals, proxies, mezzanines, audios
    fileName
  ].join('/');
}
```

2. **Document lifecycle policy:**

Create `apps/studio-api/docs/r2-lifecycle-policy.md`:

```markdown
# R2 Lifecycle Policy

## Storage Classes

- **Originals:** Standard storage, indefinite retention
- **Mezzanines:** Standard storage, 90 days (or project completion + 30 days)
- **Proxies:** Standard storage, indefinite (cheap, enables UI)
- **Audio WAVs:** Standard storage, 30 days (regenerate on demand)

## Retention Rules

Apply via Cloudflare dashboard or API:

1. Delete mezzanines after 90 days
2. Delete audio WAVs after 30 days
3. Keep originals and proxies indefinitely
```

**Acceptance Criteria:**
- [ ] Given sessionId + cameraLabel, can compute all expected paths
- [ ] Paths follow naming convention (no spaces, ISO 8601 dates)
- [ ] Lifecycle policy documented and ready to apply

**Files:**
- `apps/studio-api/src/services/storage/object-naming.ts`
- `apps/studio-api/docs/r2-lifecycle-policy.md`

---

### Track C: Upload + Ingestion Service

**Deliverable:** API endpoints for upload initialization and ingestion pipeline

**Implementation:**

1. **API Gateway Routes:**

```typescript
// apps/studio-api/src/routes/v2/sessions.ts

import { Router } from 'express';

const router = Router();

// POST /v2/sessions - Create new session
router.post('/', async (req, res) => {
  const { title, date, location, notes, projectId } = req.body;

  // Create session in Strapi
  const session = await strapi.entityService.create('api::recording-session.recording-session', {
    data: { title, date, location, notes, projectId, status: 'draft' }
  });

  res.status(201).json(session);
});

// POST /v2/sessions/:id/assets/init-upload
router.post('/:id/assets/init-upload', async (req, res) => {
  const { cameraLabel, filename, contentType, sizeBytes } = req.body;

  // Generate R2 presigned upload URL (multipart if >5GB)
  const { uploadUrl, uploadId, parts } = await generateUploadUrl({
    sessionId: req.params.id,
    cameraLabel,
    filename,
    sizeBytes
  });

  // Create asset record in Strapi
  const asset = await strapi.entityService.create('api::media-asset.media-asset', {
    data: {
      session: req.params.id,
      cameraLabel,
      originalFilename: filename,
      status: 'uploading'
    }
  });

  res.json({ assetId: asset.id, uploadUrl, uploadId, parts });
});

// POST /v2/sessions/:id/assets/:assetId/complete
router.post('/:id/assets/:assetId/complete', async (req, res) => {
  const { uploadId, parts } = req.body;

  // Complete multipart upload on R2
  await completeMultipartUpload(uploadId, parts);

  // Update asset status and queue ingestion
  await strapi.entityService.update('api::media-asset.media-asset', req.params.assetId, {
    data: { status: 'ingesting' }
  });

  // Queue background job for transcode/extraction
  await queueIngestionJob({ assetId: req.params.assetId });

  res.json({ status: 'ingesting' });
});

export default router;
```

2. **Ingestion Worker:**

```typescript
// apps/studio-api/src/workers/ingestion-worker.ts

import ffmpeg from 'fluent-ffmpeg';

export async function processAsset(assetId: string) {
  const asset = await strapi.entityService.findOne('api::media-asset.media-asset', assetId);

  // 1. Extract metadata
  const metadata = await extractMetadata(asset.originalUrl);
  await strapi.entityService.update('api::media-asset.media-asset', assetId, {
    data: { durationMs: metadata.duration, fps: metadata.fps, resolution: metadata.resolution }
  });

  // 2. Extract audio to WAV
  const audioUrl = await extractAudio(asset.originalUrl, assetId);
  await createDerivedMedia(assetId, 'audio-wav', audioUrl);

  // 3. Generate proxy
  const proxyUrl = await generateProxy(asset.originalUrl, assetId);
  await createDerivedMedia(assetId, 'proxy', proxyUrl);

  // 4. Generate mezzanine (optional for now)
  // const mezzanineUrl = await generateMezzanine(asset.originalUrl, assetId);
  // await createDerivedMedia(assetId, 'mezzanine', mezzanineUrl);

  // 5. Mark asset ready
  await strapi.entityService.update('api::media-asset.media-asset', assetId, {
    data: { status: 'ready' }
  });
}

async function extractAudio(sourceUrl: string, assetId: string): Promise<string> {
  const outputKey = `sessions/${assetId}/audio/${assetId}.wav`;

  return new Promise((resolve, reject) => {
    ffmpeg(sourceUrl)
      .noVideo()
      .audioCodec('pcm_s16le')
      .audioChannels(2)
      .audioFrequency(48000)
      .on('end', () => resolve(uploadToR2AndGetUrl(outputKey)))
      .on('error', reject)
      .save(`/tmp/${assetId}.wav`);
  });
}

async function generateProxy(sourceUrl: string, assetId: string): Promise<string> {
  const outputKey = `sessions/${assetId}/proxy/${assetId}.mp4`;

  return new Promise((resolve, reject) => {
    ffmpeg(sourceUrl)
      .videoCodec('libx264')
      .outputOptions([
        '-pix_fmt yuv420p',
        '-profile:v baseline',
        '-level 3.0',
        '-g 2', // CRITICAL: keyframe every 2 frames for smooth scrubbing
        '-preset fast',
        '-crf 23',
        '-movflags +faststart',
      ])
      .videoFilters('scale=-2:720') // 720p
      .noAudio() // Proxies don't need audio
      .on('end', () => resolve(uploadToR2AndGetUrl(outputKey)))
      .on('error', reject)
      .save(`/tmp/${assetId}-proxy.mp4`);
  });
}
```

**Acceptance Criteria:**
- [ ] 3 uploads yield 3 assets with derived audio + proxy
- [ ] Ingestion is idempotent (retries don't duplicate assets)
- [ ] Metadata extracted correctly (duration, fps, resolution)
- [ ] R2 paths follow naming convention

**Files:**
- `apps/studio-api/src/routes/v2/sessions.ts`
- `apps/studio-api/src/services/media-ingestion/upload-handler.ts`
- `apps/studio-api/src/workers/ingestion-worker.ts`
- `apps/studio-api/src/services/storage/r2-client.ts`

---

### Track D: Auto-Sync Engine

**Deliverable:** Compute offsets + confidence between cameras using audio correlation

**Implementation:**

1. **Choose master audio:**

```typescript
// apps/studio-api/src/services/sync/master-selector.ts

export async function selectMasterCamera(sessionId: string): Promise<'A' | 'B' | 'C'> {
  const assets = await strapi.entityService.findMany('api::media-asset.media-asset', {
    filters: { session: sessionId }
  });

  // Heuristic: pick camera with highest audio RMS (loudest, clearest)
  // For now, default to camera A
  // TODO: Implement audio quality analysis

  return 'A';
}
```

2. **Offset computation using audio-offset-finder:**

```typescript
// apps/studio-api/src/services/sync/sync-detector.ts

import { execFile } from 'child_process';
import { promisify } from 'util';

const execFileAsync = promisify(execFile);

interface SyncResult {
  camera: string;
  offsetMs: number;
  confidence: number;
  isReliable: boolean;
}

export async function detectSync(sessionId: string): Promise<SyncResult[]> {
  // 1. Get all assets for session
  const assets = await strapi.entityService.findMany('api::media-asset.media-asset', {
    filters: { session: sessionId },
    populate: ['derivatives']
  });

  // 2. Select master camera
  const masterCamera = await selectMasterCamera(sessionId);
  const masterAsset = assets.find(a => a.cameraLabel === masterCamera);
  const masterAudio = masterAsset.derivatives.find(d => d.kind === 'audio-wav');

  // 3. Run correlation for each non-master camera
  const results: SyncResult[] = [];

  for (const asset of assets) {
    if (asset.cameraLabel === masterCamera) {
      results.push({ camera: asset.cameraLabel, offsetMs: 0, confidence: 100, isReliable: true });
      continue;
    }

    const comparisonAudio = asset.derivatives.find(d => d.kind === 'audio-wav');

    // Download audio files from R2 to temp location
    const masterPath = await downloadToTemp(masterAudio.url);
    const comparisonPath = await downloadToTemp(comparisonAudio.url);

    // Run audio-offset-finder
    const { stdout } = await execFileAsync('audio-offset-finder', [
      '--find-offset-of', comparisonPath,
      '--within', masterPath,
      '--json'
    ]);

    const result = JSON.parse(stdout);

    // result.offset: time offset in seconds (can be negative)
    // result.standard_score: confidence (>10 = reliable, <5 = unreliable)

    results.push({
      camera: asset.cameraLabel,
      offsetMs: Math.round(result.offset * 1000), // Convert to milliseconds
      confidence: result.standard_score,
      isReliable: result.standard_score > 5
    });
  }

  // 4. Store results in sync-result
  const offsetsMs = results.reduce((acc, r) => ({ ...acc, [r.camera]: r.offsetMs }), {});
  const confidence = results.reduce((acc, r) => ({ ...acc, [r.camera]: r.confidence }), {});

  await strapi.entityService.create('api::sync-result.sync-result', {
    data: {
      session: sessionId,
      masterCamera,
      offsetsMs,
      confidence,
      method: 'xcorr',
      operatorStatus: 'pending',
      computedAt: new Date()
    }
  });

  // 5. Update session status
  const allReliable = results.every(r => r.isReliable);
  await strapi.entityService.update('api::recording-session.recording-session', sessionId, {
    data: { status: allReliable ? 'needs-review' : 'needs-review' } // Same for now, UI will differentiate
  });

  return results;
}
```

3. **Confidence classification:**

```typescript
// apps/studio-api/src/services/sync/confidence-scorer.ts

export function classifyConfidence(score: number): 'looks-good' | 'review-suggested' | 'needs-manual-nudge' {
  if (score >= 10) return 'looks-good';
  if (score >= 5) return 'review-suggested';
  return 'needs-manual-nudge';
}
```

**Acceptance Criteria:**
- [ ] For typical 3-camera recordings, stable offsets and confidence in one run
- [ ] Re-running sync with same inputs returns same result (deterministic)
- [ ] Confidence thresholds classify correctly (>=10, 5-10, <5)
- [ ] Results persisted to sync-result in Strapi

**Files:**
- `apps/studio-api/src/services/sync/master-selector.ts`
- `apps/studio-api/src/services/sync/sync-detector.ts`
- `apps/studio-api/src/services/sync/confidence-scorer.ts`

---

### Track E: Operator Preview & Manual Nudge Hooks

**Deliverable:** API endpoints for sync approval and manual correction

**Implementation:**

1. **Preview API:**

```typescript
// apps/studio-api/src/routes/v2/sessions.ts (continued)

// GET /v2/sessions/:id/sync
router.get('/:id/sync', async (req, res) => {
  const syncResult = await strapi.entityService.findMany('api::sync-result.sync-result', {
    filters: { session: req.params.id }
  });

  if (!syncResult || syncResult.length === 0) {
    return res.status(404).json({ error: 'Sync not computed yet' });
  }

  const result = syncResult[0];

  // Add classification for each camera
  const classification = Object.keys(result.confidence).reduce((acc, camera) => ({
    ...acc,
    [camera]: classifyConfidence(result.confidence[camera])
  }), {});

  res.json({
    sessionId: req.params.id,
    masterCamera: result.masterCamera,
    offsets: result.offsetsMs,
    confidence: result.confidence,
    classification,
    operatorStatus: result.operatorStatus,
    computedAt: result.computedAt
  });
});

// POST /v2/sessions/:id/sync/approve
router.post('/:id/sync/approve', async (req, res) => {
  const { approvedBy, notes } = req.body;

  const syncResult = await strapi.entityService.findMany('api::sync-result.sync-result', {
    filters: { session: req.params.id }
  });

  await strapi.entityService.update('api::sync-result.sync-result', syncResult[0].id, {
    data: {
      operatorStatus: 'approved',
      approvedAt: new Date()
    }
  });

  await strapi.entityService.update('api::recording-session.recording-session', req.params.id, {
    data: { status: 'ready' }
  });

  res.json({
    sessionId: req.params.id,
    status: 'ready',
    operatorStatus: 'approved',
    finalOffsets: syncResult[0].offsetsMs
  });
});

// POST /v2/sessions/:id/sync/correct
router.post('/:id/sync/correct', async (req, res) => {
  const { correctedBy, offsets, notes } = req.body;

  const syncResult = await strapi.entityService.findMany('api::sync-result.sync-result', {
    filters: { session: req.params.id }
  });

  await strapi.entityService.update('api::sync-result.sync-result', syncResult[0].id, {
    data: {
      operatorStatus: 'corrected',
      operatorOffsetsMs: offsets,
      approvedAt: new Date()
    }
  });

  await strapi.entityService.update('api::recording-session.recording-session', req.params.id, {
    data: { status: 'ready' }
  });

  res.json({
    sessionId: req.params.id,
    status: 'ready',
    operatorStatus: 'corrected',
    finalOffsets: offsets,
    originalOffsets: syncResult[0].offsetsMs
  });
});
```

**Acceptance Criteria:**
- [ ] Operator can view sync results with confidence scores
- [ ] Operator can approve with one click
- [ ] Operator can adjust offsets and persist corrections
- [ ] Session status becomes "ready" after approval/correction

**Files:**
- `apps/studio-api/src/routes/v2/sessions.ts` (add sync endpoints)

---

## Testing Plan

See `09-05-TEST-PLAN.md` for comprehensive test scenarios.

**Critical Tests:**
- [ ] **Golden path:** Upload 3 videos → auto-sync (confidence >= 10) → approve → ready
- [ ] **Low confidence:** Upload 3 videos → auto-sync (confidence < 5) → manual nudge → ready
- [ ] **Edge cases:** VFR detection, missing audio, different sample rates, retry idempotency

---

## Definition of Done

Phase 9 is complete when:

- [ ] Can ingest 3-camera session end-to-end
- [ ] Proxies available for preview (smooth scrubbing)
- [ ] Sync offsets + confidence produced and persisted
- [ ] Operator can approve or correct offsets
- [ ] Final offsets stored for Phase 10/11 consumption
- [ ] All acceptance criteria met across tracks A-E
- [ ] Code committed with proper git history
- [ ] Phase 9 summary document created

---

## Implementation Notes

**Idempotency:** Use sessionId + cameraLabel + derived-kind as unique keys to prevent duplicates.

**Checksums:** Store checksums for derived files to detect corruption/rebuild needs.

**Sync artifacts:** Keep correlation plots small (<100KB JSON) for instant UI loads.

**Proxy quality:** "Good enough" now, optimize later. -g 2 is the critical setting.

**VFR handling:** Detect with ffprobe, convert to CFR before sync to avoid drift.

---

## Research References

All technical decisions informed by `09-RESEARCH.md`:
- **Standard stack:** FFmpeg, audio-offset-finder, R2, fluent-ffmpeg
- **Architecture patterns:** 4 detailed patterns with code examples
- **Common pitfalls:** 6 catalogued with warning signs
- **What NOT to hand-roll:** BBC audio-offset-finder (don't build custom MFCC correlation)

---

## Next Steps

After Phase 9 completion:
- **Phase 10:** Transcript alignment
- **Phase 11:** EDL generation from sync offsets
- **Phase 12:** Remotion rendering using mezzanines
- **Phase 13:** Studio UI for operator workflows
